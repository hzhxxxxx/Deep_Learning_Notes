
- 1
![[_- visual selection (1).png]]


- 2，权重初始化的重要性
	
	中心思想是：*初始权重不能设为0*或随意设置因为它们直接影响各层激活值的分布。为了确保神经网络能高效学习,关键在于*保持各层激活值分布的适当广度*。因此,对于每一层与上一层神经元连接的权重,其初始化至关重要：示意图应分别展示两种常见初始化方法的核心思想：
	
	1:Xavier 初始化：适用场景：当需要初始化与上一层`n` 个神经元连接的权重时,且激活函数为 Sigmoid。核心思想：将这些初始权重值设定为标准差为 `1/√n` 的分布,旨在保持激活值分布的稳定性。
	
	2:He 初始化：适用场景：当需要初始化与上一层 `n` 个神经元连接的权重时，且激活函数为 ReLU。核心思想：将这些初始权重值设定为标准差为 `√(2/n)` 的分布,专门针对ReLU的特性。



- 3，*Batch Normalization算法*
	
	Batch Normalization 旨在调节各层的激活值分布，使其拥有适当的广度，从而优化神经网络的学习过程。
	
	Batch Normalization 的特点和工作原理：
	
	Batch Normalization 的作用和优势：
    
    - 促使学习过程快速进行。
    - 降低对初始值设定的敏感度。
    - 有助于抑制过拟合。
    
	Batch Normalization 的工作方式：
    
    - 在神经网络的每个隐藏层到其对应的激活函数之间，插入一个“标准分布化的检查站”（即 Batch Normalization 层）。
    - 这个检查站在对输入数据进行标准化（例如，使其均值为0，方差为1）之后，还会引入两个可学习的参数：缩放参数（通常表示为 γ）和偏移参数（通常表示为 β）。
    - 这两个参数允许网络在标准化的基础上，对激活值进行灵活的缩放和偏移，从而自行学习和调整最佳的激活值分布。



- 4，处理*过拟合*问题，提高*泛化能力*
	
	*过拟合*：指的是只能拟合训练数据，但不能很好地拟合不包括在训练数据中的其它数据的状态。即泛化能力不行。
	
	1，*权值衰减*：一种抑制过拟合的方法，
	本身的思想起点是：如果某个权重过大，那么代表着模型的结果会非常依赖于该特征进而做出预测。
	方法是：新损失函数=原损失函数+λ × （所有权重的平方和）
	这个λ是一个自己设定的超参数，决定了权值衰减的强度。
	有点类似于“超调“的概念，抑制过渡学习
	
	2，*Dropout*：训练时随机地”关闭“一些神经元，迫使团队中的其它神经元学会独立思考，更加独立，鲁棒化。
	（dropout更”集成学习“这个概念有关，集成学习就是让多个模型单独进行学习，推理的时候再取多个模型的输出平均值，而dropout每一次随机关闭神经元就可以看作多个不同的模型进行学习。）




- 5，*超参数的验证*：
	其实就是数据除了要分为1训练数据，2测试数据，之外还得分出一个”*验证数据*“，用这个进行超参数的好坏验证，不用测试数据，因为要防止模型过早地接触考试真题。
	（这个验证数据有时候数据会分，也可也自己手动地从训练数据中分一部分出来，作为验证数据）
	*超参数的最优化*：
	1，设定一个超参数的大概范围
	2，从设定的超参数范围中随机采样
	3，使用采样到的超参数值进行学习，通过验证数据评估识别精度。
	4，重复上面步骤，根据他们的识别精度结果，不断缩小超参数的范围。